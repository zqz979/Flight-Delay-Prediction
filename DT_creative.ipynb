{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import pandas as pd\n",
    "\n",
    "df = load_data(separate=False)\n",
    "df_stock_only = df[['AIR_STOCK','NASDAQ', 'DOW', 'ARR_DEL15']]\n",
    "\n",
    "\n",
    "# get the string columns\n",
    "cat_cols = df.drop(columns=['ARR_DEL15', 'DISTANCE', 'NASDAQ', 'DOW', 'AIR_STOCK']).columns.tolist()\n",
    "\n",
    "\n",
    "# Fit and transform the encoder on the selected columns\n",
    "encoder = OrdinalEncoder()\n",
    "encoded_cols = encoder.fit_transform(df[cat_cols])\n",
    "\n",
    "# Convert the encoded columns to a DataFrame\n",
    "encoded_df = pd.DataFrame(encoded_cols, columns=cat_cols)\n",
    "\n",
    "# Drop the original columns from the DataFrame\n",
    "df = df.drop(columns=cat_cols)\n",
    "\n",
    "# Concatenate the encoded columns with the remaining columns\n",
    "df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make balanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Determine the class with the fewer samples.\n",
    "counts = df_stock_only['ARR_DEL15'].value_counts()\n",
    "minority_class = counts.idxmin()\n",
    "minority_count = counts[minority_class]\n",
    "\n",
    "# Select a random sample of the larger class with the same size as the smaller class.\n",
    "majority_class = 1 - minority_class\n",
    "majority_count = counts[majority_class]\n",
    "majority_sample = df_stock_only[df_stock_only['ARR_DEL15'] == majority_class].sample(n=minority_count, random_state=0)\n",
    "\n",
    "# Combine the two classes into a single DataFrame.\n",
    "minority_df = df_stock_only[df_stock_only['ARR_DEL15'] == minority_class]\n",
    "balanced_df_stock_only = pd.concat([minority_df, majority_sample], axis=0)\n",
    "\n",
    "minority_df = df[df['ARR_DEL15'] == minority_class]\n",
    "majority_sample = df[df['ARR_DEL15'] == majority_class].sample(n=minority_count, random_state=0)\n",
    "balanced_df = pd.concat([minority_df, majority_sample], axis=0)\n",
    "\n",
    "balanced_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(balanced_df_stock_only.drop(columns=['ARR_DEL15']), balanced_df_stock_only['ARR_DEL15'], test_size=0.2, stratify=balanced_df_stock_only['ARR_DEL15'], random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'max_depth': [i for i in range(1,25 ,1)]}\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, verbose=3, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search object to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best depth found by the grid search\n",
    "print(\"Best depth:\", grid_search.best_params_['max_depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, PrecisionRecallDisplay\n",
    "\n",
    "# Train the decision tree classifier\n",
    "clf = DecisionTreeClassifier(max_depth = grid_search.best_params_['max_depth'])\n",
    "# clf = DecisionTreeClassifier()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = clf.predict(X_test)\n",
    "  \n",
    "# Calculate the accuracy of the predictions\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print('Tree Depth: ', clf.get_depth())\n",
    "print('Number of leaves: ', clf.get_n_leaves())\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "# print the report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "display = PrecisionRecallDisplay.from_predictions(y_test, y_pred)\n",
    "_ = display.ax_.set_title(\"Precision-Recall curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_importances = pd.Series(clf.feature_importances_, index=X_train.columns)\n",
    "feature_importances.sort_values().plot(kind='barh')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Create a random forest classifier object\n",
    "rf = RandomForestClassifier(n_estimators = 1000, n_jobs=-1, verbose=2, random_state=0)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "print('Accuracy Score:', accuracy_score(y_test, y_pred))\n",
    "print('Classification Report:\\n', classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosted tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# Instantiate the model with default hyperparameters\n",
    "gbc = HistGradientBoostingClassifier(max_iter = 10000, verbose=1, n_iter_no_change = 100, random_state=0)\n",
    "\n",
    "# Fit the model to the training data\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = gbc.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "print('Accuracy Score:', accuracy_score(y_test, y_pred))\n",
    "print('Classification Report:\\n', classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(balanced_df.drop(columns=['ARR_DEL15']), balanced_df['ARR_DEL15'], test_size=0.2, stratify=balanced_df['ARR_DEL15'], random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'max_depth': [i for i in range(1,46)]}\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, verbose=3, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search object to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best depth found by the grid search\n",
    "print(\"Best depth:\", grid_search.best_params_['max_depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, PrecisionRecallDisplay\n",
    "\n",
    "# Train the decision tree classifier\n",
    "clf = DecisionTreeClassifier(max_depth = grid_search.best_params_['max_depth'])\n",
    "# clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = clf.predict(X_test)\n",
    "  \n",
    "# Calculate the accuracy of the predictions\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print('Tree Depth: ', clf.get_depth())\n",
    "print('Number of leaves: ', clf.get_n_leaves())\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "# print the report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "display = PrecisionRecallDisplay.from_predictions(y_test, y_pred)\n",
    "_ = display.ax_.set_title(\"Precision-Recall curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "feature_importances = pd.Series(clf.feature_importances_, index=X_train.columns)\n",
    "feature_importances.sort_values().plot(kind='barh')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Create a random forest classifier object\n",
    "rf = RandomForestClassifier(n_estimators = 1000, n_jobs=-1, verbose=2, random_state=0)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "print('Accuracy Score:', accuracy_score(y_test, y_pred))\n",
    "print('Classification Report:\\n', classification_report(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosted tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# Instantiate the model with default hyperparameters\n",
    "gbc = HistGradientBoostingClassifier(max_iter = 10000, verbose=1, n_iter_no_change = 100, random_state=0)\n",
    "\n",
    "# Fit the model to the training data\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = gbc.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "print('Accuracy Score:', accuracy_score(y_test, y_pred))\n",
    "print('Classification Report:\\n', classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
