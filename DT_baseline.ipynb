{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_bts_data\n",
    "\n",
    "df = load_bts_data(separate=False)\n",
    "df_no_str = df.select_dtypes(exclude=['object'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode string columns with ordinal encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# get the string columns\n",
    "cat_cols = df.drop(columns=['ARR_DEL15', 'DISTANCE']).columns.tolist()\n",
    "\n",
    "\n",
    "# Fit and transform the encoder on the selected columns\n",
    "encoder = OrdinalEncoder()\n",
    "encoded_cols = encoder.fit_transform(df[cat_cols])\n",
    "\n",
    "# Convert the encoded columns to a DataFrame\n",
    "encoded_df = pd.DataFrame(encoded_cols, columns=cat_cols)\n",
    "\n",
    "# Concatenate the encoded columns with the remaining columns\n",
    "df_encoded = pd.concat([df.drop(columns=cat_cols), encoded_df], axis=1)\n",
    "\n",
    "df_encoded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide into pre and post covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_covid = df_encoded[df_encoded['YEAR'].isin([0,1,2])]\n",
    "df_post_covid= df_encoded[df_encoded['YEAR'].isin([3,4])]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make balanced DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Determine the class with the fewer samples.\n",
    "counts = df_post_covid['ARR_DEL15'].value_counts()\n",
    "minority_class = counts.idxmin()\n",
    "minority_count = counts[minority_class]\n",
    "\n",
    "# Select a random sample of the larger class with the same size as the smaller class.\n",
    "majority_class = 1 - minority_class\n",
    "majority_count = counts[majority_class]\n",
    "majority_sample = df_post_covid[df_post_covid['ARR_DEL15'] == majority_class].sample(n=minority_count, random_state=0)\n",
    "\n",
    "# Combine the two classes into a single DataFrame.\n",
    "minority_df = df_post_covid[df_post_covid['ARR_DEL15'] == minority_class]\n",
    "balanced_df = pd.concat([minority_df, majority_sample], axis=0)\n",
    "\n",
    "balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# stratify make the split also balanced\n",
    "X_train, X_test, y_train, y_test = train_test_split(balanced_df.drop(columns=['ARR_DEL15']), balanced_df['ARR_DEL15'], test_size=0.2, stratify=balanced_df['ARR_DEL15'], random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "min_features_to_select = 1  # Minimum number of features to consider\n",
    "clf = DecisionTreeClassifier()\n",
    "cv = StratifiedKFold(5, shuffle=True, random_state = 0)\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=clf,\n",
    "    cv=cv,\n",
    "    scoring=\"accuracy\",\n",
    "    min_features_to_select=min_features_to_select,\n",
    "    verbose=1, \n",
    "    n_jobs=-1\n",
    ")\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Optimal number of features: {rfecv.n_features_}\")\n",
    "print(\"Selected features:\", X_train.columns[rfecv.support_])\n",
    "\n",
    "\n",
    "n_scores = len(rfecv.cv_results_[\"mean_test_score\"])\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Mean validation accuracy\")\n",
    "plt.errorbar(\n",
    "    range(min_features_to_select, n_scores + min_features_to_select),\n",
    "    rfecv.cv_results_[\"mean_test_score\"],\n",
    "    yerr=rfecv.cv_results_[\"std_test_score\"],\n",
    ")\n",
    "plt.title(\"Recursive Feature Elimination \\nwith correlated features\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'max_depth': [i for i in range(1,39 ,1)]}\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, verbose=3, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search object to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best depth found by the grid search\n",
    "print(\"Best depth:\", grid_search.best_params_['max_depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, PrecisionRecallDisplay\n",
    "\n",
    "# Train the decision tree classifier\n",
    "clf = DecisionTreeClassifier(max_depth=grid_search.best_params_['max_depth'])\n",
    "# clf = DecisionTreeClassifier()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = clf.predict(X_test)\n",
    "  \n",
    "# Calculate the accuracy of the predictions\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print('Tree Depth: ', clf.get_depth())\n",
    "print('Number of leaves: ', clf.get_n_leaves())\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "# print the report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "display = PrecisionRecallDisplay.from_predictions(y_test, y_pred)\n",
    "_ = display.ax_.set_title(\"Precision-Recall curve\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.Series(clf.feature_importances_, index=X_train.columns)\n",
    "feature_importances.sort_values().plot(kind='barh')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Create a random forest classifier object\n",
    "rf = RandomForestClassifier(n_estimators = 1000, n_jobs=-1, verbose=2, random_state=0)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "print('Accuracy Score:', accuracy_score(y_test, y_pred))\n",
    "print('Classification Report:\\n', classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosted tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# Instantiate the model with default hyperparameters\n",
    "gbc = HistGradientBoostingClassifier(max_iter = 10000, verbose=1, n_iter_no_change = 100, random_state=0)\n",
    "\n",
    "# Fit the model to the training data\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = gbc.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the model\n",
    "print('Accuracy Score:', accuracy_score(y_test, y_pred))\n",
    "print('Classification Report:\\n', classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "835a06f688b698c7d614088d7afc5bde9bf71fddaf4dd29b1587dcde18254742"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
